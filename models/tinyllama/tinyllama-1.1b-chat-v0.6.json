{
    "_version": "1.0.0",
    "id": "TinyLlama/TinyLlama-1.1B-Chat-v0.6",
    "name": "tinyllama-1.1b-chat-v0.6",
    "creator": "TinyLlama",
    "title": "TinyLlama-1.1B",
    "version": "0.6",
    "description": "This is the chat model finetuned on top of TinyLlama/TinyLlama-1.1B-intermediate-step-955k-2T. We follow HF's Zephyr's training recipe. The model was initially fine-tuned on a variant of the UltraChat dataset, which contains a diverse range of synthetic dialogues generated by ChatGPT. We then further aligned the model with ðŸ¤— TRL's DPOTrainer on the openbmb/UltraFeedback dataset, which contain 64k prompts and model completions that are ranked by GPT-4.",
    "icon": "https://opla.github.io/models/assets/tinnyllama.png",
    "license": "Apache-2.0",
    "tags": "llama",
     "task_type": ["conversational"],
    "languages": [
        "en"
    ],
    "model_size": "1.1b",
    "context_size": 1024,
    "tensor_type": "float32",
    "base_model": "TinyLlama/TinyLlama-1.1B-intermediate-step-955k-2T",
    "model_type": "llama",
    "library": "PyTorch",
    "featured": true,
    
    "repository": "https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v0.6",
    "include": [
        {
            "name": "TinyLlama-1.1B-Chat-v0.6.q4_0.gguf",
            "base_model": "TinyLlama/TinyLlama-1.1B-Chat-v0.6",
            "library": "GGUF",
            "quantization": "Q4_0",
            "size": 0.637,
            "sha": "ec21b060225c96d9a985886566c2d01d0c63498405f4e0448dfee0491d73219f",
            "file_size": 636725760,
            "download": "https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v0.6/resolve/main/ggml-model-q4_0.gguf"
        }
    ]
}