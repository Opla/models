{
    "_version": "1.0.0",
    "name": "tinyllama-1.1b-chat-v0.6",
    "creator": "TinyLlama",
    "title": "TinyLlama-1.1B",
    "version": "0.6",
    "description": "This is the chat model finetuned on top of TinyLlama/TinyLlama-1.1B-intermediate-step-955k-2T. We follow HF's Zephyr's training recipe. The model was initially fine-tuned on a variant of the UltraChat dataset, which contains a diverse range of synthetic dialogues generated by ChatGPT. We then further aligned the model with ðŸ¤— TRL's DPOTrainer on the openbmb/UltraFeedback dataset, which contain 64k prompts and model completions that are ranked by GPT-4.",
    "license": "Apache-2.0",
    "tags": "llama",
     "task_type": ["conversational"],
    "languages": [
        "en"
    ],
    "model_size": "1.1b",
    "context_size": 1024,
    "tensor_type": "float32",
    "base_model": "TinyLlama/TinyLlama-1.1B-intermediate-step-955k-2T",
    "model_type": "llama",
    "library": "PyTorch",
    "featured": true,
    
    "repository": "https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v0.6",
    "include": [
        {
            "name": "Llama-2-7B-Chat-GGML",
            "description": "This repo contains GGUF format model files for Meta Llama 2's Llama 2 7B Chat.",
            "base_model": "TinyLlama/TinyLlama-1.1B-Chat-v0.6",
            "library": "GGUF",
            "quantization": "Q4_0",
            "download": "https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v0.6/blob/main/ggml-model-q4_0.gguf"
        }
    ]
}